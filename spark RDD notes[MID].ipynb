{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkSession using Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "Spark has 3 structure:\\\n",
    "    **RDD(Resilient Distributed Dataset)**\\\n",
    "        schema free, low-level, unstructured, read-only\\\n",
    "    **DataFrame**\\\n",
    "        structured, schema\\\n",
    "    **DataSet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating RDD from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6, 7]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separating list into three rdd using glom\n",
    "spark.sparkContext.parallelize([1,2,3,4,5,6,7], 3).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating RDD from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**writing a file into data folder**\n",
    "\n",
    "%%writefile data/ds_spark_wiki.txt\\\n",
    " Wikipedia\n",
    " Apache Spark is an open source cluster computing framework.\n",
    " 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    " Apache Spark Apache Spark Apache Spark Apache Spark\n",
    " 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    " Originally developed at the University of California, Berkeley's AMPLab,\n",
    " the Spark codebase was later donated to the Apache Software Foundation,\n",
    " which has maintained it since.\n",
    " Spark provides an interface for programming entire clusters with\n",
    " implicit data parallelism and fault-tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating RDD from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35, 2',\n",
       " '40, 27',\n",
       " '12, 38',\n",
       " '15, 31',\n",
       " '21, 1',\n",
       " '14, 19',\n",
       " '46, 1',\n",
       " '10, 34',\n",
       " '28, 3',\n",
       " '48, 1',\n",
       " '16, 2',\n",
       " '30, 3',\n",
       " '32, 2',\n",
       " '48, 1',\n",
       " '31, 2',\n",
       " '22, 1',\n",
       " '12, 3',\n",
       " '39, 29',\n",
       " '19, 37',\n",
       " '25, 2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map(fn)\\\n",
    "요소별로 fn을 적용해서 결과 RDD 돌려줌\\\n",
    "**rdd.map(lambda x: x.split(' '))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[9] at RDD at PythonRDD.scala:53\n",
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print(squared)\n",
    "#since the return of map is RDD, to see the actual result .collect() is needed\n",
    "print(squared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "str.split() 함수에 인자가 없으면 whitespace로 분리한다.\\\n",
    "whitespace는 공백이나 탭 등의 기호를 말하는 것으로, 문장을 분리해서 단어로 분리할 경우에 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자 개수 세기\n",
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대소문자 변환\n",
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print (upperRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foreach vs map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreach 반환값X\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map 반환값O\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .filter(fn)\\\n",
    "요소별로 선별하여 fn을 적용해서 결과 RDD 돌려줌\\\n",
    "**rdd.filter(lambda x: \"Spark\" in x)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Spark\" 단어가 포함된 문장이 조건이된다. count() 함수로 그 갯수를 확인해 보자.\n",
    "\n",
    "#English spark\n",
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print (\"How many lines having 'Spark': \",myRdd_spark.count())\n",
    "#한글 스파크\n",
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print (myRdd_unicode.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .flatMap(fn)\\\n",
    "요소별로 fn을 적용하고, flat해서 결과 RDD 돌려줌\\\n",
    "**rdd.flatMap(lambda x: x.split(' '))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split())\\\n",
    "                    .filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupBy()\\\n",
    "Paired, unpaired RDD에 모두 사용할 수 있지만, 주로 unpaired RDD에 많이 쓰인다.\\\n",
    "paied 는 (key, value) 쌍으로 구성된 데이터, unpaired 는 (key,value)쌍으로 구성되지 않은 데이터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', <pyspark.resultiterable.ResultIterable at 0x1f67d279948>),\n",
       " ('Busan', <pyspark.resultiterable.ResultIterable at 0x1f67d279908>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupBy 함수를 적용한 것은 리스트로 변환하여 값을 확인해야한다.\\\n",
    "- mapValues 사용하여 (key1, key2) 형식으로 리스트 만듦\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()\n",
    "#_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupByKey()\\\n",
    "key를 그룹해서 iterator를 돌려줌.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipeline\\\n",
    "함수를 연이어 적용하는 방식\\\n",
    "rdd.map().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving RDD to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partition 늘리기\n",
    "_testRdd=spark.sparkContext.parallelize(_testList, 2)\n",
    "#parition 개수 확인\n",
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key민 출력하기\n",
    "_testRdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pair RDD\\\n",
    "groupByKey()\\\n",
    "reduceByKey()\\\n",
    "combineByKey()\\\n",
    "aggregateByKey()\\\n",
    "mapValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey()\n",
    "동일한 key에 대해 value의 합계를 구한다. (parition 별로 작업 수행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 6), ('key2', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()\n",
    "mapValues를 해야만 결과를 직접적으로 확인가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', <pyspark.resultiterable.ResultIterable at 0x1f67d267408>),\n",
       " ('key2', <pyspark.resultiterable.ResultIterable at 0x1f67d2675c8>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', [1, 1, 1, 1, 1, 1]), ('key2', [1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1),\n",
       " ('Apache', 6),\n",
       " ('Spark', 7),\n",
       " ('is', 1),\n",
       " ('an', 2),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(10)\n",
    "#def f(x): return len(x)\n",
    "#=mapValues(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey()\n",
    "reduceByKey()는 groupByKey()와 달리 키별로 빈도를 합산하기 때문에 mapValues()가 필요없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1),\n",
       " ('Apache', 6),\n",
       " ('Spark', 7),\n",
       " ('is', 1),\n",
       " ('an', 2),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()\n",
    "reduceByKey() 는 (K,V)로 countByKey()는 dictionary로 병합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Wikipedia': 1,\n",
       "             'Apache': 6,\n",
       "             'Spark': 7,\n",
       "             'is': 1,\n",
       "             'an': 2,\n",
       "             'open': 1,\n",
       "             'source': 1,\n",
       "             'cluster': 1,\n",
       "             'computing': 1,\n",
       "             'framework.': 1,\n",
       "             '아파치': 5,\n",
       "             '스파크는': 1,\n",
       "             '오픈': 1,\n",
       "             '소스': 1,\n",
       "             '클러스터': 1,\n",
       "             '컴퓨팅': 1,\n",
       "             '프레임워크이다.': 1,\n",
       "             '스파크': 4,\n",
       "             'Originally': 1,\n",
       "             'developed': 1,\n",
       "             'at': 1,\n",
       "             'the': 3,\n",
       "             'University': 1,\n",
       "             'of': 1,\n",
       "             'California,': 1,\n",
       "             \"Berkeley's\": 1,\n",
       "             'AMPLab,': 1,\n",
       "             'codebase': 1,\n",
       "             'was': 1,\n",
       "             'later': 1,\n",
       "             'donated': 1,\n",
       "             'to': 1,\n",
       "             'Software': 1,\n",
       "             'Foundation,': 1,\n",
       "             'which': 1,\n",
       "             'has': 1,\n",
       "             'maintained': 1,\n",
       "             'it': 1,\n",
       "             'since.': 1,\n",
       "             'provides': 1,\n",
       "             'interface': 1,\n",
       "             'for': 1,\n",
       "             'programming': 1,\n",
       "             'entire': 1,\n",
       "             'clusters': 1,\n",
       "             'with': 1,\n",
       "             'implicit': 1,\n",
       "             'data': 1,\n",
       "             'parallelism': 1,\n",
       "             'and': 1,\n",
       "             'fault-tolerance.': 1})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .countByKey()\n",
    "#.items() to be added to get a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제: 단어 빈도 세기 \n",
    "내림차순으로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "\n",
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey()\n",
    "(key, (sum, count))\\\n",
    "각 첫 번째 요소에 대해서는 첫 번째 함수(combiner)를 적용하지만,\\\n",
    "똑같은 key가 존재한다면 두 번째 함수(merge value)를 적용하고,\\\n",
    "parition이 여러개라면 세 번째 함수(merge combiners)를 적용해서 merge한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_testList=[(\"key1\",1),(\"key1\",3),(\"key2\",2),(\"key1\",2),(\"key2\",4),\n",
    "           (\"key1\",5),(\"key2\",6),\n",
    "           (\"key1\",7),(\"key1\",8),(\"key2\",9),(\"key2\",3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#3#2#5#7#8'), ('key2', '2*#4#6#9#3')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parition이 1개일 때, 세번째 함수는 작동하지 않음\n",
    "_testRdd.combineByKey(lambda v : str(v)+\"*\", lambda c, v : c+\"#\"+str(v), lambda c1, c2 : c1+'&'+c2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 0 -> [('key1', 1), ('key1', 3), ('key2', 2), ('key1', 2), ('key2', 4)]\n",
      "Partitions 1 -> [('key1', 5), ('key2', 6), ('key1', 7), ('key1', 8), ('key2', 9), ('key2', 3)]\n"
     ]
    }
   ],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_testList, 2)\n",
    "\n",
    "partitions = _testRdd.glom().collect()\n",
    "for num, partition in enumerate(partitions):\n",
    "    print(f'Partitions {num} -> {partition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#3#2&5*#7#8'), ('key2', '2*#4&6*#9#3')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parition2개 일때, 3번째 함수 적용\n",
    "_testRdd.combineByKey(lambda v : str(v)+\"*\", lambda c, v : c+\"#\"+str(v), lambda c1, c2 : c1+'&'+c2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', (26, 6)), ('key2', (24, 5))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각 key에 대해서 value 더하기\n",
    "_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),\n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 4.333333333333333, 'key2': 4.8}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각 key 별로 average 구하기\n",
    "_testCbkRdd=_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),                      \n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "averageByKey = _testCbkRdd.map(lambda x:(x[0],x[1][0]/x[1][1]))\n",
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kim', (252, 3)), ('lim', (336, 4)), ('lee', (99, 1))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])\n",
    "\n",
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])\n",
    "\n",
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 179.5, 'F': 163.0}\n"
     ]
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda x: (x[0],x[1][0]/x[1][1]))\n",
    "\n",
    "print (avgByKey.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제: 성적합계 및 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 답1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"'김하나','English', 100\",\n",
    "    \"'김하나','Math', 80\",\n",
    "    \"'임하나','English', 70\",\n",
    "    \"'임하나','Math', 100\",\n",
    "    \"'김갑돌','English', 82.3\",\n",
    "    \"'김갑돌','Math', 98.5\"\n",
    "]\n",
    "_marksRdd=spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 3-1: 이름으로 합계를 구해보자. 올바른 출력은 다음과 같다. 이름과 점수로 데이터를 추출하고, 이름별로 (이름을 키로) 합계를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marks by name\n",
    "_marksbyname=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[0],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' 180.0\n",
      "'임하나' 170.0\n",
      "'김갑돌' 180.8\n"
     ]
    }
   ],
   "source": [
    "for i in _marksbyname:\n",
    "  print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 3-2: 개인별 데이터를 컴마로 분리하고, 과목 x[1]과 성적 x[2]만 꺼내어 reduceBykey()를 구하면 합계를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'English' 252.3\n",
      "'Math' 278.5\n"
     ]
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksbysubject=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()\n",
    "for i in _marksbysubject:\n",
    "  print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 3-3: 합계, 개수를 계산해 보자. combineByKey()를 이용해서 계산해야 한다. 먼저 데이터를 이름, 과목, 데이터 -> 이름, 점수로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marks by name\n",
    "_marksbyname2=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[0],float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' (180.0, 2) \n",
      "'임하나' (170.0, 2) \n",
      "'김갑돌' (180.8, 2) \n"
     ]
    }
   ],
   "source": [
    "# sum, counts by name\n",
    "sum_counts = _marksbyname2.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print (each, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 3-4: 개인별 평균은 3-3에서 구했던 합계, 개수를 사용하여 계산한다. 평균을 계산하기 위해 float() 형변환을 해주었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' 90.0 \n",
      "'임하나' 85.0 \n",
      "'김갑돌' 90.4 \n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "averageByKey = sum_counts\\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .collect()\n",
    "for i in averageByKey:\n",
    "    for j in i:\n",
    "        print (j, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
