{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL , Extract, Transform, Load\n",
    "\n",
    "| pacakge | use   | DataType   |\n",
    "|------|------|------|\n",
    "|   mllib  | RDD API| pyspark.mllib.linalg.Vector / pyspark.mllib.linalg.Matrix|\n",
    "|   ml  | DataFrame API| pyspark.ml.linalg.Vector /pyspark.ml.linalg.Matrix|\n",
    "\n",
    "### RDD 변환\n",
    "\n",
    "| 구분 | descript   |\n",
    "|------|------|\n",
    "|   Vector  | numpy vector와 같은 기능을 한다. dense와 sparse vector로 구분한다.|\n",
    "|   Labeled Point  | 분류를 의미하는 클래스 또는 label과 속성 features 이 묶인 구조로서, 지도학습 supervised learning을 할 경우 사용된다.|\n",
    "|   Matrix  | \tnumpy matrix와 같은 특징을 가진다.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors\n",
    "- dense vector : 모든 행렬이 값을 가지고 있다.\n",
    "- sparse vector : 빈 값이 많아서, 값이 있는 경우 그 값이 있는 인덱스로 표현해 배열을 축약. sparse는 실제 값이 없는 요소, '0'을 제거하여 만든 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector: [1.0,2.1,3.0]\n",
      "Type: <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])\n",
    "print (\"Dense vector: {}\\nType: {}\".format(dv1mllib, type(dv1mllib)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml의 dense vector: [1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])\n",
    "print (\"ml의 dense vector: {}\".format(dv1ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.SparseVector"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse vector\n",
    "sv1 = Vectors.sparse(5,[0,1,4],[160.0,69.0,24.0])\n",
    "type(sv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#matrix to sparse vector \n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print(mtx.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [2., 3., 0., 0.],\n",
       "       [0., 0., 5., 0.],\n",
       "       [0., 4., 6., 0.],\n",
       "       [0., 0., 7., 0.],\n",
       "       [0., 0., 0., 8.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "#(6,4)\n",
    "\n",
    "dm = Matrices.dense(6,4,[1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 8])\n",
    "dm.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(6, 4, [0, 2, 4, 7, 8], [0, 1, 1, 3, 2, 3, 4, 5], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toSparse()\n",
    "#열로 개수세서 누적 합, 행으로 행의 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeled Point\n",
    "\n",
    "S.4.4 Labeled Point\n",
    "Labeled point는 로컬벡터로 레이블을 가지고 있는 밀집 또는 희소 행렬을 말한다. 레이블이 있으므로, supervised learning에 요구되는 형식이다. double 형식으로 저장되어 있어야한다. \n",
    "\n",
    "|구분| 지도학습을 하기 위한 label과 features의 구성  | \n",
    "|------|------|\n",
    "|   label  | \tsupervised learning에서 '구분 값'으로 사용한다. 데이터타입은 'DoubleType'으로 설정되어야 한다.|\n",
    "|   features  | \tsparse, dense 모두 사용할 수 있다.| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF(Term Frequency)\n",
    "단어 빈도를 계산하기 위해 HasingTF를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rdd생성, \n",
    "import os\n",
    "wikiRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.0}),\n",
       " SparseVector(1048576, {148618: 1.0, 183975: 1.0, 216207: 1.0, 261052: 1.0, 617454: 1.0, 696349: 1.0, 721336: 1.0, 816618: 1.0, 897662: 1.0}),\n",
       " SparseVector(1048576, {60386: 1.0, 177421: 1.0, 568609: 1.0, 569458: 1.0, 847171: 1.0, 850510: 1.0, 1040679: 1.0}),\n",
       " SparseVector(1048576, {261052: 4.0, 816618: 4.0}),\n",
       " SparseVector(1048576, {60386: 4.0, 594754: 4.0}),\n",
       " SparseVector(1048576, {21980: 1.0, 70882: 1.0, 274690: 1.0, 357784: 1.0, 549790: 1.0, 597434: 1.0, 804583: 1.0, 829803: 1.0, 935701: 1.0}),\n",
       " SparseVector(1048576, {154253: 1.0, 261052: 1.0, 438276: 1.0, 460085: 1.0, 585459: 1.0, 664288: 1.0, 816618: 1.0, 935701: 2.0, 948143: 1.0, 1017889: 1.0}),\n",
       " SparseVector(1048576, {270017: 1.0, 472985: 1.0, 511771: 1.0, 718483: 1.0, 820917: 1.0}),\n",
       " SparseVector(1048576, {34116: 1.0, 87407: 1.0, 276491: 1.0, 348943: 1.0, 482882: 1.0, 549350: 1.0, 721336: 1.0, 816618: 1.0, 1025622: 1.0}),\n",
       " SparseVector(1048576, {1769: 1.0, 151357: 1.0, 500659: 1.0, 547760: 1.0, 979482: 1.0})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(wikiRdd3)\n",
    "tf.collect() #희소 행렬로, row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.7047}),\n",
       " SparseVector(1048576, {148618: 1.7047, 183975: 1.7047, 216207: 1.7047, 261052: 1.0116, 617454: 1.7047, 696349: 1.7047, 721336: 1.2993, 816618: 0.7885, 897662: 1.7047}),\n",
       " SparseVector(1048576, {60386: 1.2993, 177421: 1.7047, 568609: 1.7047, 569458: 1.7047, 847171: 1.7047, 850510: 1.7047, 1040679: 1.7047}),\n",
       " SparseVector(1048576, {261052: 4.0464, 816618: 3.1538}),\n",
       " SparseVector(1048576, {60386: 5.1971, 594754: 6.819}),\n",
       " SparseVector(1048576, {21980: 1.7047, 70882: 1.7047, 274690: 1.7047, 357784: 1.7047, 549790: 1.7047, 597434: 1.7047, 804583: 1.7047, 829803: 1.7047, 935701: 1.2993}),\n",
       " SparseVector(1048576, {154253: 1.7047, 261052: 1.0116, 438276: 1.7047, 460085: 1.7047, 585459: 1.7047, 664288: 1.7047, 816618: 0.7885, 935701: 2.5986, 948143: 1.7047, 1017889: 1.7047}),\n",
       " SparseVector(1048576, {270017: 1.7047, 472985: 1.7047, 511771: 1.7047, 718483: 1.7047, 820917: 1.7047}),\n",
       " SparseVector(1048576, {34116: 1.7047, 87407: 1.7047, 276491: 1.7047, 348943: 1.7047, 482882: 1.7047, 549350: 1.7047, 721336: 1.2993, 816618: 0.7885, 1025622: 1.7047}),\n",
       " SparseVector(1048576, {1769: 1.7047, 151357: 1.7047, 500659: 1.7047, 547760: 1.7047, 979482: 1.7047})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.collect() #결과는 같지만, tf가 아니라 idf가 들어감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "데이터를 표준화하려면 1) 평균과 표준편차를 계산하고, 2) 측정값에서 평균을 빼고, 표준편차로 나누어 주면 된다. 즉 zscore를 계산하는 것과 같다.\n",
    "\n",
    "$$ z = \\frac {\\bar{x_n} - \\mu} {\\sigma / \\sqrt{n}} $$\n",
    "\n",
    "- from pyspark.mllib.feature import StandardScaler (Rdd)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(_tRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 변환\n",
    "기계학습에 넘겨줄 입력데이터를 형식에 맞추어야 함.\n",
    "특징 추출하여 feature vectors를 구성, 지도 학습에서는 class 또는 label 필요\\\n",
    "Labeled Point를 label, features 컬럼으로 분해\n",
    "RDD LabeledPoint는 label과 vectors로 구성되어 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label, features를 갖고 있는 데이터\n",
    "#LabeledPoint는 RDD에서 사용하는 구조로서 mllib 라이브러리를 사용\n",
    "#(DataFrame은 LabeledPoint를 컬럼으로 가지고 있지 않는다.)\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mllib labeledPoing을 거치지 않으면, 컬럼명이 _1,_2처럼 출력\\\n",
    "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\\\n",
    " Row(_1=1, _2=[1.1, 2.1, 3.1]),\\\n",
    " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n",
    "_df=_rdd.toDF()\n",
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#직접 컬럼명 변경 -> 기계학습\n",
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')\n",
    "_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어빈도\n",
    "텍스트는 정량데이터가 아니기 때문에, 단어의 빈도에 따라 정량화하여 과학적인 분석을 하게 된다.\n",
    "\n",
    "텍스트 변환 단계\n",
    "텍스트를 변환하는 단계를 보자. 순서는 변경될 수 있다.\n",
    "\n",
    "-단계 1: 단어로 분할 Tokenization\n",
    "그, 영화는, 매우, 강렬했다, 그냥, 좋았다, 영화관에서, 보는, 동안, 긴장을, 늦출, 수, 없었다, 갑돌이가, 분장한, 악당의, 케릭터가, 만들어지는, 과정은, 흥미롭지, 않을, 수가, 없었다, 무비의, 이야기, 전개는, 빠르고, 무엇이, 진실이고, 거짓인지, 판단할, 수, 없었다, 누가, 이런, 영화를, 좋아, 하지, 않을, 수가, 있겠는가, 이모티콘\n",
    "\n",
    "-단계 2: 정리\\\n",
    "불필요, 오타 등\n",
    "\n",
    "-단계 3: 불용어 stopwords 제거\\\n",
    "그, 수, 수가, 수, 이런, 하지, 수가 등\n",
    "\n",
    "-단계 4: 어간 추출 stemming 영화는, 영화의는 다른 단어지만 조사를 제거하면 동일한 단어\\\n",
    "좋았다, 좋아 단어들은 어근을 판별하면 동일한 단어이다.\n",
    "영화, 무비의 단어는 이음동의\n",
    "\n",
    "-단계 5: 계량화\n",
    "word vector로 만든다.\n",
    "있다-없다, 단어빈도, TF-IDF 사용할 수 있다.\n",
    "dense, sparse 모두 가능하다. [1,1,1,1,1,0,0],[0,1,0,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "- corpus는 어떤 주제에 대해 쓰여지거나, 어떤 사람이 작성한 전체 '말뭉치'를 말한다. 여러 문장으로 구성된 텍스트 집합을 말한다.\n",
    "- document는 문장으로 구성된 문서를 말하지만, 한 문장으로만 구성될 수도, 여러 문장으로 만들어질 수도 있다. 예를 들어, \"why she had to go\" 같은 한 문장도 document라고 하고, \"why she had to go.. I don't know\" 역시 마찬가지이다.\n",
    "- vocabularay는 중복없는 단어 집합을 말하며, 예를 들면, \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\" 등은 단어이다.\\\n",
    "Tokenizer는 document를 단어로 분리한다. 분리하는 기준은 whitespace로 공백, TAB, CR, New Line 등이 해당된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|      우리 Let it be|\n",
      "|        나 Let it be|\n",
      "|        너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])\n",
    "myDf.show(truncate = True) #truncate=true, 자르지 않고 모두 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer= Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "tokDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegTokenizer\n",
    "Tokenizer는 white space로 분리하지만, RegexTokenizer는 단어를 분리하기 위해 정규표현식을 적용할 수 있다. 정규표현식을 사용하여 분리하거나 특정 패턴을 추출할 수 있다. 공백으로 분리할 경우 간단히 정규표현식 \\s 패턴을 적용할 수 있다. 한글에는 \\w 패턴이 적용되지 않는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "re= RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "#\\\\s+ 공백이 있을 때 분리하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords \n",
    "의미 없는 불필요한 단어들, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_7ee8c47fda11"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")\n",
    "\n",
    "#how to get stopwords\n",
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "#add my stopWords\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "CountVectorizer는 단어의 빈도 수를 계산한다.\\\n",
    "hasingTF와 같은 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer 는 1차원 입력만 가능함.\n",
    "#2차원을 1차원으로 만들어주는 함수\n",
    "from functools import reduce \n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense vector 로 출력\n",
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark CountVectorizer\n",
    "\n",
    "- minDf는 너무 적게 발생하는 경우 무시, 예를 들어 0.5는 전체 문서의 50%보다 적게 발생하는 단어는 무시, 1.0은 기본 값이고, 100%보다 적게 발생하는 경우 무시하게 된다. 즉, minDf=1.0은 문서 1개 이하에서 나타난 단어는 무시하라는 의미이다. 즉 어떤 단어도 무시하지 말라는 의미이다.\n",
    "- maxDf는 너무 많이 발생하는 경우 무시, 예를 들어 0.5는 전체 문서의 50%보다 많이 발생하는 경우 무시, 1.0은 100%보다 많이 발생하는 경우 무시 (즉, 어떤 단어도 무시하지 말라는 의미). min_df와 마찬가지로 1.0이 기본 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)\n",
    "\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "cvDf.show(3)\n",
    "\n",
    "#일부 컬럼만 선정해서 출력\n",
    "#cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF 계산\n",
    "IDF는 자주 나타나는 단어에 대한 가중치를 줄이고, 드물게 나타나는 단어에 가중치를 높이는 방식으로 계산된다.\n",
    "\n",
    "| 항목 | 설명   | example   |\n",
    "|------|------|------|\n",
    "|   tf(d,f)  | 단어 t가 문서 d에서 나타나는 단어의 빈도 수, term frequency| $f_{t,d}$ / (number of words in d) = 1/4 = 0.25\n",
    "(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)|\n",
    "|   df  |document frequency 단어가 나타난 문서 수| 3 (wisdom이 포함된 문서는 3)|\n",
    "|N|number of documents 전체 문서의 수|11 (전체의 문서는 11개)|\n",
    "|idf|inverse document frequency 단어가 나타난 문서의 비율을 거꾸로|ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1,, 0으로 나뉘는 것을 방지하기 위해 smoothing, 즉 1을 더한다.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#max_df = 1.0 -> 어떤 단어도 무시하지 말라는 의미\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 14)\t2.791759469228055\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark를 사용한 TF-IDF\n",
    "HashingTF에서의 numFeatures는 $2^n$으로 결정함.\n",
    "기본은 $2^{18}=262,144$이다. 너무 적게 설정되면 인덱스가 부족하거나 적절하게 매핑될 수 있으니 주의해야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32) #  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")\n",
    "hashDf = hashTF.transform(stopDf) #hashingTF는 fit하지 않고 transform()\n",
    "\n",
    "#앞서 hashTF의 결과는 벡터튜플이고, 이를 IDF에 입력으로 넣어준다.\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "\n",
    "idfModel = idf.fit(hashDf)\n",
    "\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             nostops|                hash|\n",
      "+--------------------+--------------------+\n",
      "|[find, times, tro...|(262144,[64317,91...|\n",
      "|[mother, mary, co...|(262144,[24657,63...|\n",
      "|[speaking, words,...|(262144,[27556,15...|\n",
      "|    [hour, darkness]|(262144,[74517,98...|\n",
      "|[standing, right,...|(262144,[84798,21...|\n",
      "|[speaking, words,...|(262144,[27556,15...|\n",
      "|               [let]|(262144,[173339],...|\n",
      "|               [let]|(262144,[173339],...|\n",
      "|               [let]|(262144,[173339],...|\n",
      "|               [let]|(262144,[173339],...|\n",
      "|[whisper, words, ...|(262144,[151864,1...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfDf.select('nostops','hash').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word 2 Vec\n",
    "벡터('king') - 벡터('man') + 벡터('woman') = 벡터('queen) 이런 연산이 가능해진다. 즉 king 단어벡터에서 man 단어백터를 빼고, woman 단어백터를 더하면, queen 단어백터를 구할 수 있다는 의미이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount= 0, inputCol=\"words\", outputCol = \"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([0.0441, -0.008, -0.0792]))\n",
      "Row(w2v=DenseVector([0.0468, -0.0011, 0.0295]))\n",
      "Row(w2v=DenseVector([0.0224, 0.0343, -0.0383]))\n"
     ]
    }
   ],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------------+\n",
      "|word    |vector                                                           |\n",
      "+--------+-----------------------------------------------------------------+\n",
      "|trouble |[0.11287034302949905,0.1447676420211792,-0.07580796629190445]    |\n",
      "|mother  |[0.13211971521377563,0.026881175115704536,-0.14194709062576294]  |\n",
      "|find    |[-0.15181413292884827,-0.004126318264752626,-0.13149575889110565]|\n",
      "|standing|[0.16076406836509705,0.13818997144699097,-0.09662546962499619]   |\n",
      "|wisdom, |[0.14846765995025635,0.11940699815750122,-0.018271425738930702]  |\n",
      "|in      |[0.04129142314195633,0.010938539169728756,-0.13256393373012543]  |\n",
      "|myself  |[0.08741272240877151,0.07993830740451813,0.0181463323533535]     |\n",
      "|is      |[0.10587336122989655,-0.03098754584789276,-0.09800274670124054]  |\n",
      "|darkness|[-0.12206508964300156,-0.1522350162267685,-0.05019150301814079]  |\n",
      "|우리    |[0.15934604406356812,0.05628468841314316,0.1532307267189026]     |\n",
      "|front   |[-0.11841410398483276,0.11112254858016968,0.15610679984092712]   |\n",
      "|it      |[0.10959182679653168,0.10470131039619446,0.03082684427499771]    |\n",
      "|너      |[-0.029491975903511047,-0.1594364196062088,0.015327585861086845] |\n",
      "|she     |[-0.1057354062795639,-0.13596655428409576,-0.05557955056428909]  |\n",
      "|comes   |[-0.1313765048980713,0.1250576376914978,0.08893169462680817]     |\n",
      "|i       |[0.1284378319978714,-0.0027535371482372284,-0.1092335507273674]  |\n",
      "|hour    |[0.13326354324817657,-0.15123650431632996,-0.08215784281492233]  |\n",
      "|to      |[-0.04798367992043495,-0.08574345707893372,0.1198904886841774]   |\n",
      "|speaking|[-0.03375723585486412,-0.09197686612606049,-0.15462493896484375] |\n",
      "|mary    |[0.13214868307113647,-0.1463983803987503,0.02257581800222397]    |\n",
      "+--------+-----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#숫자 해석 불가, 다만 워드 임베딩해서 얻은 의미 있는 숫자들\n",
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "| word|        similarity|\n",
      "+-----+------------------+\n",
      "|   너|0.9580903053283691|\n",
      "|   나|0.8260308504104614|\n",
      "|right|0.8046327829360962|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"times\", 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "#2-gram -> bigram\n",
    "ngram = NGram(n=2, inputCol = \"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StringIndexer\n",
    "문자열 컬럼을 인덱스 컬럼으로 변환\\\n",
    "빈도가 제일 높은 순서로 0.0부터 인덱스 값이 주어진다. 인덱스는 double 형을 가지게 된다. 없는 레이블에 대해서는 예외가 발생할 수 있으므로 (default), setHandleInvalid(\"skip\") 함수로 'skip', 'keep', 'error' 등으로 설정할 수 있다.\n",
    "\n",
    "| 구분 | descript | example|\n",
    "|------|------|------|\n",
    "|   nominal  | 명목 또는 구분 값 cateogry |사자, 호랑이, 사람|\n",
    "|   ordinal  | 명목값과 다른 점은 순서가 있다.| 키, low,med,high|\n",
    "|   interval  | 일정한 간격이 있다.|150-165,165-180,180-195|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      5.0|\n",
      "|Mother Mary comes...|      3.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      1.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      9.0|\n",
      "|        나 Let it be|      7.0|\n",
      "|        너 Let it be|      8.0|\n",
      "|           Let it be|      2.0|\n",
      "|Whisper words of ...|      6.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model = labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "|11.0| 66.49|127.45|    0.0|\n",
      "|12.0| 67.62|114.14|    0.0|\n",
      "|13.0|  68.3|125.61|    1.0|\n",
      "|14.0| 67.12|122.46|    0.0|\n",
      "|15.0| 68.28|116.09|    1.0|\n",
      "|16.0| 71.09| 140.0|    1.0|\n",
      "|17.0| 66.46| 129.5|    0.0|\n",
      "|18.0| 68.65|142.97|    1.0|\n",
      "|19.0| 71.23| 137.9|    1.0|\n",
      "|20.0| 67.13|124.04|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show()\n",
    "#68보다 크면 1 작으면 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)\n",
    "#3등분으로 나눔."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+---------+\n",
      "|  id|weight|height|weight2|height3| features|\n",
      "+----+------+------+-------+-------+---------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|[1.0,0.0]|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|[1.0,2.0]|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|[1.0,1.0]|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|(2,[],[])|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|(2,[],[])|\n",
      "|11.0| 66.49|127.45|    0.0|    1.0|[0.0,1.0]|\n",
      "|12.0| 67.62|114.14|    0.0|    0.0|(2,[],[])|\n",
      "|13.0|  68.3|125.61|    1.0|    1.0|[1.0,1.0]|\n",
      "|14.0| 67.12|122.46|    0.0|    0.0|(2,[],[])|\n",
      "|15.0| 68.28|116.09|    1.0|    0.0|[1.0,0.0]|\n",
      "|16.0| 71.09| 140.0|    1.0|    2.0|[1.0,2.0]|\n",
      "|17.0| 66.46| 129.5|    0.0|    1.0|[0.0,1.0]|\n",
      "|18.0| 68.65|142.97|    1.0|    2.0|[1.0,2.0]|\n",
      "|19.0| 71.23| 137.9|    1.0|    2.0|[1.0,2.0]|\n",
      "|20.0| 67.13|124.04|    0.0|    1.0|[0.0,1.0]|\n",
      "+----+------+------+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#여러 컬럼을 합쳐서 하나의 feature column으로 만들어 줄 때 사용\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.show() #feature column 벡터로 만들어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262144,[74920,89...|\n",
      "|  0.0|(262144,[89530,14...|\n",
      "|  1.0|(262144,[36803,17...|\n",
      "|  0.0|(262144,[132966,1...|\n",
      "|  0.0|(262144,[1074,389...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#위에 3개의 작업을 한 번에 함.\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)\n",
    "myDf.select('label', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
